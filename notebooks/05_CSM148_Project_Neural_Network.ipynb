{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dhw3RPAEnZc"
      },
      "source": [
        "# Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COLAB LINK: https://colab.research.google.com/drive/19sDAyPWmMXs0ks-TrHS3KakK4H1Ca0Mv?usp=sharing"
      ],
      "metadata": {
        "id": "sdlLOw6njZq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply a neural network to our Twitter financial news sentiment project.  \n",
        "Each tweet or headline is represented with a FinancialBERT embedding.  \n",
        "The neural network takes these embeddings as input and predicts a 3 class sentiment label: Neutral, Bullish, or Bearish.\n"
      ],
      "metadata": {
        "id": "QeLEcaTp7Ob1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O4bopFvnkTx-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_ok = pd.read_csv(\"balanced_training_embed.csv\", engine=\"python\")\n",
        "print(\"Training rows:\", len(train_ok))\n",
        "\n",
        "valid_ok = pd.read_csv(\"valid_embed.csv\", engine=\"python\")\n",
        "print(\"Validation rows:\", len(valid_ok))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av7v5W6agWXj",
        "outputId": "71e82ba6-e113-4ad2-dc74-2ecb676f41bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training rows: 9543\n",
            "Validation rows: 2388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyiyG2aCNF5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963f83fa-e08e-415d-81ea-35ddf2a9ba76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training shape: (9543, 10)\n",
            "Validation shape: (2388, 10)\n"
          ]
        }
      ],
      "source": [
        "training_twitter = pd.read_csv(\"training_embed.csv\", engine=\"python\")\n",
        "testing_twitter  = pd.read_csv(\"valid_embed.csv\", engine=\"python\")\n",
        "\n",
        "print(\"Training shape:\", training_twitter.shape)\n",
        "print(\"Validation shape:\", testing_twitter.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA - Dimensionality  Reduction\n",
        "\n",
        "## Data and feature representation\n",
        "\n",
        "We use the training and validation splits provided for the Twitter Financial News Sentiment dataset.\n",
        "\n",
        "- clean_text contains the cleaned tweet or headline.  \n",
        "- label is the sentiment label for each row.  \n",
        "- financialBERT_embedding is a string representation of the FinancialBERT embedding for that text.\n",
        "\n",
        "We will convert the embedding strings into numeric vectors and then reduce the dimensionality with PCA before feeding them to the neural network.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mrq6iQijOfnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create training dataframe to apply PCA:"
      ],
      "metadata": {
        "id": "8xA_Y19I5BdH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9-D4WDnVa_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c66c16e-dabb-482f-844d-e6d07525e45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training embeddings shape: (9543, 768)\n"
          ]
        }
      ],
      "source": [
        "training_df = pd.DataFrame()\n",
        "training_df[\"clean_text\"] = training_twitter[\"clean_text\"]\n",
        "training_df[\"label\"] = training_twitter[\"label\"]\n",
        "\n",
        "training_df[\"embeddings\"] = training_twitter[\"financialBERT_embedding\"].apply(\n",
        "    lambda x: np.array(x.strip(\"[]\").split(), dtype=float)\n",
        ")\n",
        "\n",
        "training_embeddings = np.vstack(training_df[\"embeddings\"].values)\n",
        "print(\"Training embeddings shape:\", training_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create testing dataframe to apply PCA:"
      ],
      "metadata": {
        "id": "YVCsDIre5FBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new dataframe that has the clean text and financial BERT embeddings\n",
        "testing_df = pd.DataFrame()\n",
        "testing_df[\"clean_text\"] = testing_twitter[\"clean_text\"]\n",
        "testing_df[\"label\"] = testing_twitter[\"label\"]\n",
        "\n",
        "# Convert embeddings into a numpy array (original uses spaces to separate)\n",
        "testing_df[\"embeddings\"] = testing_twitter[\"financialBERT_embedding\"].apply(\n",
        "    lambda x: np.array(x.strip(\"[]\").split(), dtype=float)\n",
        ")\n",
        "\n",
        "# Create single array of all the embeddings\n",
        "testing_embeddings = np.vstack(testing_df[\"embeddings\"].values)\n",
        "print(\"Validation embeddings shape:\", testing_embeddings.shape)\n"
      ],
      "metadata": {
        "id": "6xYGq15t5IHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c156ae2-4e44-4cfe-85d8-f26f5c3d93ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation embeddings shape: (2388, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-mnRd06bgsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b7a679-2991-4fe0-b389-55129848f64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced training PCA embeddings shape: (9543, 100)\n",
            "Reduced validation PCA embeddings shape: (2388, 100)\n"
          ]
        }
      ],
      "source": [
        "# Reduce dimensionality of embeddings with PCA\n",
        "pca = PCA(n_components=100)\n",
        "training_reduced_embeddings = pca.fit_transform(training_embeddings)\n",
        "testing_reduced_embeddings = pca.transform(testing_embeddings)\n",
        "\n",
        "print(\"Reduced training PCA embeddings shape:\", training_reduced_embeddings.shape)\n",
        "print(\"Reduced validation PCA embeddings shape:\", testing_reduced_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Neural Network\n"
      ],
      "metadata": {
        "id": "cAPVfLVb06YS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA on FinancialBERT embeddings\n",
        "\n",
        "FinancialBERT embeddings are high dimensional.  \n",
        "To make training more efficient, we apply PCA and keep 100 principal components.\n",
        "\n",
        "- training_reduced_embeddings is the result of fitting PCA on the training embeddings and transforming them.  \n",
        "- We then use the same PCA transformation on the validation embeddings.  \n",
        "\n",
        "This keeps most of the variance in the data while reducing the number of input features for the neural network.\n"
      ],
      "metadata": {
        "id": "_qQ2N4Xi7vdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "F9Rn-nIU1Akg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode string labels as integers 0, 1, 2 for PyTorch\n",
        "label_encoder = LabelEncoder()\n",
        "training_df[\"label_encoded\"] = label_encoder.fit_transform(training_df[\"label\"])\n",
        "testing_df[\"label_encoded\"] = label_encoder.transform(testing_df[\"label\"])\n",
        "\n",
        "print(\"Classes:\", list(label_encoder.classes_))\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(training_reduced_embeddings, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(training_df[\"label_encoded\"].values, dtype=torch.long)\n",
        "\n",
        "X_val_tensor = torch.tensor(testing_reduced_embeddings, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(testing_df[\"label_encoded\"].values, dtype=torch.long)\n",
        "\n",
        "X_train_tensor.shape, y_train_tensor.shape, X_val_tensor.shape, y_val_tensor.shape"
      ],
      "metadata": {
        "id": "jODGVwsj3OHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1579127-f0bb-44b6-9671-a714cffa4074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: [np.int64(0), np.int64(1), np.int64(2)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([9543, 100]),\n",
              " torch.Size([9543]),\n",
              " torch.Size([2388, 100]),\n",
              " torch.Size([2388]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network architecture\n",
        "\n",
        "We use a feedforward neural network for multi class classification. We train the model with cross entropy loss and stochastic gradient descent."
      ],
      "metadata": {
        "id": "Lm_nOfZ_76na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(100, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = SimpleNN()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n"
      ],
      "metadata": {
        "id": "l9Bq-3_x09Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 100\n",
        "model.train()\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    logits = model(X_train_tensor)\n",
        "    loss = loss_fn(logits, y_train_tensor)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    # Print loss every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsrSWPFE8LEI",
        "outputId": "d8d7513c-9325-40f9-8205-c21a4d1ee48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 1.0162\n",
            "Epoch [20/100], Loss: 0.9846\n",
            "Epoch [30/100], Loss: 0.9581\n",
            "Epoch [40/100], Loss: 0.9357\n",
            "Epoch [50/100], Loss: 0.9169\n",
            "Epoch [60/100], Loss: 0.9013\n",
            "Epoch [70/100], Loss: 0.8882\n",
            "Epoch [80/100], Loss: 0.8772\n",
            "Epoch [90/100], Loss: 0.8679\n",
            "Epoch [100/100], Loss: 0.8599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training procedure and hyperparameters\n",
        "\n",
        "Training details:\n",
        "\n",
        "- Optimizer: stochastic gradient descent  \n",
        "- Learning rate: 0.01  \n",
        "- Loss function: cross entropy loss  \n",
        "- Epochs: 100  \n",
        "- Batch size: full batch (we use all training examples in each update)\n",
        "\n",
        "To select the learning rate, we tried several values in a small range  \n",
        "(0.001, 0.01, 0.1) and monitored validation accuracy.  \n",
        "A learning rate of 0.01 converged reliably and gave better validation performance  \n",
        "than 0.001, while 0.1 was unstable, so we kept 0.01 for this check in.\n",
        "\n",
        "We also chose 100 epochs because the loss curve flattened out by that point and  \n",
        "additional training did not significantly improve validation accuracy."
      ],
      "metadata": {
        "id": "ZMkYoPXg8OYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COLAB LINK: https://colab.research.google.com/drive/19sDAyPWmMXs0ks-TrHS3KakK4H1Ca0Mv?usp=sharing"
      ],
      "metadata": {
        "id": "yBMwkeyejdRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Performance Metrics on Test Dataset"
      ],
      "metadata": {
        "id": "TlpT5nsofeaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_logits = model(X_val_tensor)\n",
        "    val_preds = torch.argmax(val_logits, dim=1)\n",
        "\n",
        "# Convert tensors to numpy arrays for sklearn metrics\n",
        "y_val_true = y_val_tensor.numpy()\n",
        "y_val_pred = val_preds.numpy()\n",
        "\n",
        "val_accuracy = accuracy_score(y_val_true, y_val_pred)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    y_val_true, y_val_pred, average=\"weighted\"\n",
        ")\n",
        "\n",
        "print(\"Validation accuracy:\", val_accuracy)\n",
        "print(\"Validation precision (weighted):\", precision)\n",
        "print(\"Validation recall (weighted):\", recall)\n",
        "print(\"Validation F1 (weighted):\", f1)\n",
        "\n",
        "print(\"\\nClassification report (validation):\")\n",
        "target_names = [str(c) for c in label_encoder.classes_]\n",
        "print(classification_report(y_val_true, y_val_pred, target_names=target_names))\n",
        "\n",
        "print(\"\\nConfusion matrix (validation):\")\n",
        "print(confusion_matrix(y_val_true, y_val_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGRLik1Q8QvW",
        "outputId": "69de1031-3258-4c8e-ed47-cbb3ae94b0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 0.6557788944723618\n",
            "Validation precision (weighted): 0.43004595843539306\n",
            "Validation recall (weighted): 0.6557788944723618\n",
            "Validation F1 (weighted): 0.5194485324955582\n",
            "\n",
            "Classification report (validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       347\n",
            "           1       0.00      0.00      0.00       475\n",
            "           2       0.66      1.00      0.79      1566\n",
            "\n",
            "    accuracy                           0.66      2388\n",
            "   macro avg       0.22      0.33      0.26      2388\n",
            "weighted avg       0.43      0.66      0.52      2388\n",
            "\n",
            "\n",
            "Confusion matrix (validation):\n",
            "[[   0    0  347]\n",
            " [   0    0  475]\n",
            " [   0    0 1566]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COLAB LINK: https://colab.research.google.com/drive/19sDAyPWmMXs0ks-TrHS3KakK4H1Ca0Mv?usp=sharing"
      ],
      "metadata": {
        "id": "hX36MxtIjcUl"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}